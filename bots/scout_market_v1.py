import os, re, json, time, sqlite3
import os

def _load_persona_from_env():
  core=os.environ.get("CORE_PERSONA_FILE")
  role=os.environ.get("PERSONA_FILE")
  t=[]
  if core and os.path.exists(core):
    t.append(open(core,"r",encoding="utf-8").read().strip())
  if role and os.path.exists(role):
    t.append(open(role,"r",encoding="utf-8").read().strip())
  return "\n\n".join([x for x in t if x])

PERSONA=_load_persona_from_env()

from dataclasses import dataclass
from typing import List, Dict, Any, Optional
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from openai import OpenAI

from oclibs.telegram import send as tg_send

load_dotenv()
client = OpenAI()
MODEL = os.getenv("OPENAI_MODEL", "gpt-4.1-mini")

HEADERS = {"User-Agent": "openclaw-factory/0.1 (+contact: douyo01-byte)"}
DB_PATH = "data/openclaw.db"

EMAIL_RE = re.compile(r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}")

@dataclass
class Item:
    source: str
    title: str
    url: str
    summary: str = ""
    japan_presence: str = "unknown"  # likely/unlikely/unknown
    score: int = 0
    rationale_jp: str = ""
    category_guess: str = ""
    emails: List[str] = None
    contact_urls: List[str] = None

    def __post_init__(self):
        self.emails = self.emails or []
        self.contact_urls = self.contact_urls or []

def db() -> sqlite3.Connection:
    os.makedirs("data", exist_ok=True)
    return sqlite3.connect(DB_PATH)

def is_seen(url: str) -> bool:
    con = db()
    cur = con.cursor()
    cur.execute("SELECT 1 FROM items WHERE url=? LIMIT 1", (url,))
    row = cur.fetchone()
    con.close()
    return row is not None

def upsert_item(it: Item) -> None:
    con = db()
    cur = con.cursor()
    cur.execute("""
      INSERT INTO items(url, title, source)
      VALUES(?,?,?)
      ON CONFLICT(url) DO UPDATE SET
        last_seen_at = datetime('now'),
        title=excluded.title,
        source=excluded.source
    """, (it.url, it.title, it.source))
    con.commit()
    con.close()

def load_sources(path="configs/sources.json") -> List[Dict[str, Any]]:
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)["sources"]

def fetch_html(url: str) -> Optional[str]:
    try:
        r = requests.get(url, headers=HEADERS, timeout=30)
        if r.status_code >= 400:
            return None
        return r.text
    except Exception:
        return None

def parse_list(selector_mode: str, base_url: str, html: str) -> List[Dict[str, str]]:
    soup = BeautifulSoup(html, "xml")
    out: List[Dict[str, str]] = []

    def add(title: str, href: str, summary: str = ""):
        if not title or not href:
            return
        url = href if href.startswith("http") else urljoin(base_url, href)
        title = re.sub(r"\s+", " ", title).strip()
        summary = re.sub(r"\s+", " ", summary).strip()
        out.append({"title": title, "url": url, "summary": summary[:400]})

    # --- Kicktraq ---
    if selector_mode == "kicktraq":
        for a in soup.select("a[href]"):
            href = a.get("href","")
            txt = a.get_text(" ", strip=True)
            # projects/xxxx/xxxx/ ã®å½¢ã‚’æ‹¾ã†
            if "/projects/" in href and txt and len(txt) > 6:
                add(txt, href)
        return dedupe(out)

    # --- Product Hunt ---
    if selector_mode == "producthunt":
        # ã–ã£ãã‚Š â€œpostâ€ ã®ãƒªãƒ³ã‚¯ã£ã½ã„ã®ã‚’æ‹¾ã†
        for a in soup.select("a[href]"):
            href = a.get("href","")
            if href.startswith("/posts/"):
                title = a.get_text(" ", strip=True)
                if title:
                    add(title, href)
        return dedupe(out)

    # --- HN Show ---
    if selector_mode == "hn_show":
        for a in soup.select("a.storylink, span.titleline > a"):
            href = a.get("href","")
            title = a.get_text(" ", strip=True)
            add(title, href)
        return dedupe(out)

    # --- BetaList ---
    if selector_mode == "betalist":
        for a in soup.select("a[href]"):
            href = a.get("href","")
            if "/startup/" in href:
                title = a.get_text(" ", strip=True)
                add(title, href)
        return dedupe(out)

    # --- JP crowdfundï¼ˆå„ã‚µã‚¤ãƒˆã¯å¾Œã§ç²¾åº¦æ”¹å–„ã€‚ã¾ãšã¯ãƒªãƒ³ã‚¯æ‹¾ã„ï¼‰ ---
    if selector_mode.startswith("jp_"):
        for a in soup.select("a[href]"):
            href = a.get("href","")
            title = a.get_text(" ", strip=True)
            # çŸ­ã™ãã‚‹ã®ã¯é™¤å¤–
            if title and len(title) >= 8:
                add(title, href)
        return dedupe(out)

    return dedupe(out)

def dedupe(items: List[Dict[str, str]]) -> List[Dict[str, str]]:
    seen = set()
    out = []
    for x in items:
        u = x["url"]
        if not u or u in seen:
            continue
        seen.add(u)
        out.append(x)
    return out

def extract_contacts(url: str) -> Dict[str, List[str]]:
    html = fetch_html(url)
    if not html:
        return {"emails": [], "contact_urls": []}

    soup = BeautifulSoup(html, "xml")
    text = soup.get_text(" ", strip=True)
    emails = sorted(set(EMAIL_RE.findall(text)))[:10]

    contact_urls = []
    for a in soup.select("a[href]"):
        href = a.get("href","").strip()
        if not href:
            continue
        h = href.lower()
        if any(k in h for k in ["contact", "support", "help", "inquiry", "about", "press"]):
            contact_urls.append(href if href.startswith("http") else urljoin(url, href))
    contact_urls = sorted(set(contact_urls))[:10]

    return {"emails": emails, "contact_urls": contact_urls}

def llm_assess(it: Item) -> Dict[str, Any]:
    prompt = f"""
ã‚ãªãŸã¯ã€Œæµ·å¤–ï¼ˆã‚¯ãƒ©ãƒ•ã‚¡ãƒ³ï¼‹ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—ï¼‰â†’æ—¥æœ¬æœªä¸Šé™¸â†’ç‹¬å è²©å£²å€™è£œã€ã‚’é¸åˆ¥ã™ã‚‹å¯©æŸ»å®˜ã§ã™ã€‚
ã‚«ãƒ†ã‚´ãƒªã¯é™å®šã—ãªã„ãŒã€ã‚¬ã‚¸ã‚§ãƒƒãƒˆ/å®¶é›»ã¯å„ªå…ˆè©•ä¾¡ã€‚
â€œè¦ç´„ã«è§¦ã‚Œã«ãã„ç™ºæ˜â€ãŒå‰æãªã®ã§ã€å±é™º/é•æ³•/è–¬æ©Ÿæ³•ç³»ã¯ä½è©•ä¾¡ã€‚

å‡ºåŠ›ã¯JSONã®ã¿ï¼š
{{
  "japan_presence": "likely|unlikely|unknown",
  "score": 0-100,
  "category_guess": "çŸ­ã",
  "rationale_jp": "æ—¥æœ¬èªã§çŸ­ãï¼ˆæ³¨æ„ç‚¹ã‚‚ï¼‰"
}}

æ¡ˆä»¶:
ã‚¿ã‚¤ãƒˆãƒ«: {it.title}
URL: {it.url}
æ¦‚è¦: {it.summary}
"""
    res = client.chat.completions.create(
        model=MODEL,
        messages=[{"role":"user","content":prompt}],
        temperature=0.2,
    )
    txt = res.choices[0].message.content.strip()
    m = re.search(r"\{.*\}", txt, re.DOTALL)
    if not m:
        return {"japan_presence":"unknown","score":0,"category_guess":"ä¸æ˜","rationale_jp":"JSONå–å¾—å¤±æ•—"}
    try:
        return json.loads(m.group(0))
    except Exception:
        return {"japan_presence":"unknown","score":0,"category_guess":"ä¸æ˜","rationale_jp":"JSONãƒ‘ãƒ¼ã‚¹å¤±æ•—"}

def format_meeting(top: List[Item], jp_trends: List[Item]) -> str:
    lines = []
    lines.append("ãƒ¤ãƒ«ãƒ‡ï¼ˆç·æ‹¬ï¼‰ğŸ§  ä¼šè­°ï¼šæµ·å¤–ç™ºæ˜â†’æ—¥æœ¬æœªä¸Šé™¸æ¨å®šâ†’é€£çµ¡å…ˆæŠ½å‡ºï¼ˆå–¶æ¥­æ–‡ã¯æœªä½œæˆï¼‰")
    lines.append("")
    lines.append("ã‚¹ã‚«ã‚¦ãƒ³ï¼šé¢¨ã¾ã‹ã›ã«æ‹¾ã†ã€‚ã‚«ãƒ†ã‚´ãƒªã¯ç¸›ã‚‰ãªã„ãŒã€ã‚¬ã‚¸ã‚§ãƒƒãƒˆ/å®¶é›»ã¯å¼·ã‚ã«è¦‹ã‚‹ã€‚")
    lines.append("")
    lines.append("==== æµ·å¤–å€™è£œ TOP ====")
    for i, it in enumerate(top, 1):
        lines.append(f"ã€å€™è£œ{i}ã€‘Score={it.score} / æ—¥æœ¬æœªä¸Šé™¸={it.japan_presence} / ç¨®åˆ¥={it.category_guess}")
        lines.append(it.title)
        lines.append(it.url)
        if it.summary:
            lines.append(f"æ¦‚è¦: {it.summary[:160]}{'â€¦' if len(it.summary)>160 else ''}")
        lines.append(f"åˆ¤æ–­: {it.rationale_jp}")
        if it.emails:
            lines.append("ãƒ¡ãƒ¼ãƒ«: " + ", ".join(it.emails))
        if it.contact_urls:
            lines.append("å•ã„åˆã‚ã›å€™è£œ: " + ", ".join(it.contact_urls[:3]))
        lines.append("â€”")

    lines.append("")
    lines.append("==== æ—¥æœ¬ã‚¯ãƒ©ãƒ•ã‚¡ãƒ³ï¼ˆå£²ã‚Œã¦ã‚‹/ç›®ç«‹ã¤é›°å›²æ°—ï¼‰ ====")
    for it in jp_trends[:5]:
        lines.append(f"- {it.title} ({it.source})")
        lines.append(f"  {it.url}")

    lines.append("")
    lines.append("æ¬¡ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼šä¸Šä½å€™è£œã®å…¬å¼ã‚µã‚¤ãƒˆContactç²¾æŸ»â†’æ³•äººæƒ…å ±â†’ç‹¬å äº¤æ¸‰å¯å¦ã‚’åˆ¤æ–­ã€‚")
    return "\n".join(lines)

def main():
    sources = load_sources()
    overseas: List[Item] = []
    jp: List[Item] = []

    for s in sources:
        html = fetch_html(s["url"])
        if not html:
            continue
        items = parse_list(s.get("selector_mode",""), s["url"], html)
        for x in items[:40]:
            it = Item(source=s["name"], title=x["title"], url=x["url"], summary=x.get("summary",""))
            # æ—¢å‡ºã‚¹ã‚­ãƒƒãƒ—ï¼ˆæ°¸ç¶šï¼‰
            if is_seen(it.url):
                continue
            upsert_item(it)

            if s.get("selector_mode","").startswith("jp_"):
                jp.append(it)
            else:
                overseas.append(it)
        time.sleep(0.6)

    if not overseas:
        _m="ãƒ¤ãƒ«ãƒ‡ï¼šæ–°è¦ã®æµ·å¤–å€™è£œãŒæ‹¾ãˆã¾ã›ã‚“ã§ã—ãŸï¼ˆæ—¢å‡ºã‚¹ã‚­ãƒƒãƒ—ãŒåŠ¹ã„ã¦ã‚‹/ã‚½ãƒ¼ã‚¹ãŒå¼±ã„å¯èƒ½æ€§ï¼‰ã€‚"
        if not _tg_msg_sent(_m):
            tg_send(_m)
            return

    # ã‚³ã‚¹ãƒˆåˆ¶å¾¡ï¼šè©•ä¾¡ã¯æœ€å¤§20ä»¶
    batch = overseas[:20]
    for it in batch:
        time.sleep(0.9)
        j = llm_assess(it)
        it.japan_presence = j.get("japan_presence","unknown")
        it.score = int(j.get("score", 0))
        it.category_guess = j.get("category_guess","")
        it.rationale_jp = j.get("rationale_jp","")

    ranked = sorted(batch, key=lambda x: x.score, reverse=True)[:5]

    # é€£çµ¡å…ˆæŠ½å‡ºã¯ä¸Šä½ã®ã¿ï¼ˆè² è·æŠ‘åˆ¶ï¼‰
    for it in ranked:
        time.sleep(1.0)
        c = extract_contacts(it.url)
        it.emails = c["emails"]
        it.contact_urls = c["contact_urls"]

    top3 = ranked[:3]
    msg = format_meeting(top3, jp)
    
    if not _tg_url_sent(top3[0].url):
        tg_send(msg)


if __name__ == "__main__":
    main()
